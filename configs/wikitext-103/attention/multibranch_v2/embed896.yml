arch: transformer_lm_multibranch_v2_wiki103_small
task: language_modeling

no-progress-bar: true

keep-last-epochs: 20
max-update: 286000
max-lr: 1.0
t-mult: 2
lr-period-updates: 270000
lr-scheduler: cosine
lr-shrink: 0.75
warmup-updates: 16000
warmup-init-lr: 1e-07
min-lr: 1e-09
optimizer: nag
lr: 0.0001
clip-norm: 0.1
criterion: adaptive_loss
max-tokens: 3072
update-freq: 3
tokens-per-sample: 3072
seed: 1
sample-break-mode: none
skip-invalid-size-inputs-valid-test: true
ddp-backend: no_c10d

fp16: false

dropout: 0.21
attention-dropout: 0.14

weight-dropout: 0.14
decoder-glu: 1
decoder-branch-type: [attn:1:448:4, dynamic:default:448:4]
conv-linear: true 

decoder-embed-dim: 896
decoder-ffn-embed-dim: 896